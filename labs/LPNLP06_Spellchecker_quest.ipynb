{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batiukmaks/NLP-Labs/blob/main/labs/LPNLP06_Spellchecker_quest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spellchecker quest\n",
        "\n",
        "–•—Ç–æ—Å—å –Ω–∞—Ä–æ–±–∏–≤ –ø–æ–º–∏–ª–æ–∫ —É –≤—ñ—Ä—à–∞—Ö –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞. –ù–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ —Ü—ñ –ø–æ–º–∏–ª–∫–∏ —ñ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "\n",
        "## –ó–∞–¥–∞—á–∞\n",
        "\n",
        "–í–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.train_text`. –¶–µ –∑–≤–∏—á–∞–π–Ω–∏–π –Ω–µ—Ä–æ–∑–º—ñ—á–µ–Ω–∏–π —Ç–µ–∫—Å—Ç. –ù–∞ –Ω—å–æ–º—É –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å. –ü—ñ–¥—ñ–π–¥–µ –±—É–¥—å-—è–∫–∞. –Ø –±–∏ —Ä–∞–¥–∏–≤ feed-forward –Ω–µ–π—Ä–æ–Ω—É –º–æ–¥–µ–ª—å –∑ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—î—é –ø–æ –ª—ñ—Ç–µ—Ä–∞—Ö, –±–æ —Ü–µ —Ç–µ, —â–æ –º–∏ –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ª–µ–∫—Ü—ñ—ó. –ê–ª–µ n-–≥—Ä–∞–º–Ω–∞ —Ç–µ–∂ –º–∞—î —Å–ø—Ä–∞—Ü—é–≤–∞—Ç–∏.\n",
        "\n",
        "–¢–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.test_items`. –ü—Ä–∏–∫–ª–∞–¥ –æ–¥–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"text\": \"–°–ø—ñ–≤–∞–ª–∏ –± –ø—Ä–æ–∑—É, —Ç–∞ –ø–æ –Ω–æ–∂–∞—Ö,\",\n",
        "  \"error_start\": 23,\n",
        "  \"error_end\": 28,\n",
        "  \"error\": \"–Ω–æ–∂–∞—Ö\",\n",
        "  \"corrections\": [\n",
        "    \"–Ω–æ–≥–∞—Ö\",\n",
        "    \"–π–æ—Ç–∞—Ö\",\n",
        "    \"—î–Ω–æ—Ç–∞—Ö\",\n",
        "    \"–Ω–æ–∂–∞—Ö\",\n",
        "    \"–Ω–æ—Ç–∞—Ö\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "`error_start` —Ç–∞ `error_end` –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç—ñ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö). –£ –¥–∞–Ω–Ω–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ, –ø–æ–º–∏–ª–∫–æ—é —î `text[23:28]`, —Ç–æ–±—Ç–æ —Å–ª–æ–≤–æ \"–Ω–æ–∂–∞—Ö\".\n",
        "\n",
        "`corrections` -- —Ü–µ —Å–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—å.\n",
        "\n",
        "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏—Ö.\n",
        "\n",
        "\n",
        "## –ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "\n",
        "–û–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ –≤ `lab.test_items` –¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –æ–¥–Ω—É –ª—ñ—Ç–µ—Ä—É –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –î–ª—è —Ü—å–æ–≥–æ –∑–Ω–∞–π–¥—ñ—Ç—å —Ä—ñ–∑–Ω–∏—Ü—é –º—ñ–∂ –ª—ñ—Ç–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞ –∑ –ø–æ–º–∏–ª–∫–æ—é (`error`) —Ç–∞ –æ–±—Ä–∞–Ω–∏–º –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º. –ù–∞–¥—Ä—É–∫—É–π—Ç–µ —Ü—é –ª—ñ—Ç–µ—Ä—É. –Ø–∫—â–æ —Å–ª–æ–≤–æ –∑ –ø–æ–º–∏–ª–∫–æ—é –Ω–∞–ø—Ä–∞–≤–¥—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–µ, –∞ —Ç–∞–∫–µ —Ç–µ–∂ –±—É–≤–∞—î, –Ω–∞–¥—Ä—É–∫—É–π—Ç–µ –ø—Ä–æ–±—ñ–ª. –ü—Ä–∏–∫–ª–∞–¥–∏:\n",
        "\n",
        "```\n",
        "Error               Correction     To print\n",
        "-------------------------------------------\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–ª—ñ—Ç        –ª\n",
        "–ø–Ω—ñ                  –ø–æ–Ω—ñ          –æ\n",
        "–±–∞–ª–ª–µ—Ç               –±–∞–ª–µ—Ç         –ª\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–≤—ñ—Ç        (space)\n",
        "```\n",
        "\n",
        "–ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —è–∫–µ –≤–∏ –ø–æ–±–∞—á–∏—Ç–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ —Ü–µ —Ä—è–¥–æ–∫ –∑ –≤—ñ—Ä—à—É –æ–¥–Ω–æ–≥–æ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤.\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∫–≤–µ—Å—Ç -- —ñ–º'—è –∞–≤—Ç–æ—Ä–∞/–∫–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ \"–Ü–º'—è –ü—Ä—ñ–∑–≤–∏—â–µ\".\n",
        "\n",
        "–ü–æ–ª–µ—Ç—ñ–ª–∏! üöÄ"
      ],
      "metadata": {
        "id": "L-Z5-FrvSNPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --ignore-installed http://nlp.band/static/pypy/lpnlp-2023.10.2-py3-none-any.whl"
      ],
      "metadata": {
        "id": "qa35VG1zj2wR",
        "outputId": "50f091f2-f974-4231-817e-25fafc003df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lpnlp\n",
        "\n",
        "lab = lpnlp.start(\n",
        "    email=\"maksym.batiuk.kn.2021@lpnu.ua\",                   # <----------- –ó–∞–ø–æ–≤–Ω—ñ—Ç—å —Ü–µ –ø–æ–ª–µ\n",
        "    lab=\"quest_spellchecker\"\n",
        "    )"
      ],
      "metadata": {
        "id": "DjClbL-Hmcaq",
        "outputId": "5767a4d6-cd14-4b64-a920-d59c5293956e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–£–¥–∞—á—ñ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YXWCGzsBknSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ú–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å\n",
        "\n",
        "–ù–∞—Ç—Ä–µ–Ω—É–π—Ç–µ —Å–≤–æ—é –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å —Ç—É—Ç"
      ],
      "metadata": {
        "id": "CkQ69irsm_Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lab.train_text)"
      ],
      "metadata": {
        "id": "HFRrhQXUOIRq",
        "outputId": "1ea5ca42-b481-40bb-9418-b5996752dae7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414268"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lab.train_text[:330])"
      ],
      "metadata": {
        "id": "DsyqC9fPnMGa",
        "outputId": "e86bbd74-690e-4eb8-e338-92777b649d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ôªø–ü–†–ò–ß–ò–ù–ù–ê\r\n",
            "\r\n",
            "–†–µ–≤–µ —Ç–∞ —Å—Ç–æ–≥–Ω–µ –î–Ω—ñ–ø—Ä —à–∏—Ä–æ–∫–∏–π,\r\n",
            "–°–µ—Ä–¥–∏—Ç–∏–π –≤—ñ—Ç–µ—Ä –∑–∞–≤–∏–≤–∞,\r\n",
            "–î–æ–¥–æ–ª—É –≤–µ—Ä–±–∏ –≥–Ω–µ –≤–∏—Å–æ–∫—ñ,\r\n",
            "–ì–æ—Ä–∞–º–∏ —Ö–≤–∏–ª—é –ø—ñ–¥—ñ–π–º–∞.\r\n",
            "–Ü –±–ª—ñ–¥–∏–π –º—ñ—Å—è—Ü—å –Ω–∞ —Ç—É –ø–æ—Ä—É\r\n",
            "–Ü–∑ —Ö–º–∞—Ä–∏ –¥–µ-–¥–µ –≤–∏–≥–ª—è–¥–∞–≤,\r\n",
            "–ù–µ–Ω–∞—á–µ —á–æ–≤–µ–Ω –≤ —Å–∏–Ω—ñ–º –º–æ—Ä—ñ,\r\n",
            "–¢–æ –≤–∏—Ä–∏–Ω–∞–≤, —Ç–æ –ø–æ—Ç–æ–ø–∞–≤.\r\n",
            "–©–µ —Ç—Ä–µ—Ç—ñ –ø—ñ–≤–Ω—ñ –Ω–µ —Å–ø—ñ–≤–∞–ª–∏,\r\n",
            "–ù—ñ—Ö—Ç–æ –Ω—ñ–≥–¥–µ –Ω–µ –≥–æ–º–æ–Ω—ñ–≤,\r\n",
            "–°–∏—á—ñ –≤ –≥–∞—é –ø–µ—Ä–µ–∫–ª–∏–∫–∞–ª–∏—Å—å,\r\n",
            "–¢–∞ —è—Å–µ–Ω —Ä–∞–∑ —É —Ä–∞–∑ —Å–∫—Ä–∏–ø—ñ–≤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "\n",
        "  def __init__(self, tokens, unk_token=\"<unk>\"):\n",
        "    self.unk_token = unk_token\n",
        "    self.unk_index = 0\n",
        "    self.tokens = [unk_token] + tokens\n",
        "    self._itos = [unk_token] + list(set(tokens))\n",
        "    self._stoi = {token: index for index, token in enumerate(self._itos)}\n",
        "\n",
        "  def stoi(self, token: str) -> int:\n",
        "    \"\"\"Return token index or `<unk>` index if `token` is not in the vocab.\n",
        "    \"\"\"\n",
        "    return self._stoi.get(token, self.unk_index)\n",
        "\n",
        "\n",
        "  def itos(self, index: int) -> str:\n",
        "    \"\"\"Return token by its `index`.\n",
        "\n",
        "    Raise LookupError if `index` is out of vocabulary range.\n",
        "    \"\"\"\n",
        "\n",
        "    return self._itos[index]\n",
        "\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self._itos)"
      ],
      "metadata": {
        "id": "SZ9gkbweJjRB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequences(tokens, context_len):\n",
        "    for i in range(context_len, len(tokens)):\n",
        "        context = tokens[i - context_len : i]\n",
        "        target = tokens[i]\n",
        "\n",
        "        yield context, target\n",
        "\n",
        "def tokenize(text):\n",
        "    return list(text.lower())\n",
        "\n",
        "def vectorize(tokens, vocab: Vocabulary):\n",
        "    return torch.tensor([vocab.stoi(token) for token in tokens], dtype=torch.long)"
      ],
      "metadata": {
        "id": "RS29tR5FLfSp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            embed_dim,\n",
        "            context_len,\n",
        "            hidden_dim,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_len = context_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.W = nn.Linear(context_len * embed_dim, hidden_dim)\n",
        "        self.U = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, X_indices):\n",
        "        X = self.embed(X_indices)\n",
        "        e = X.view(-1, self.context_len * self.embed_dim)\n",
        "\n",
        "        h = torch.tanh(self.W(e)) # TODO: Consider other activation functions\n",
        "        logits = self.U(h)\n",
        "\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "H7nqH5ZiJ_kw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_it(sequences, batch_size, vocab):\n",
        "    batch_X = []\n",
        "    batch_y = []\n",
        "    for context, target in sequences:\n",
        "        batch_X.append(vectorize(context, vocab))\n",
        "        batch_y.append(vocab.stoi(target))\n",
        "\n",
        "        if len(batch_X) == batch_size:\n",
        "            # Pad the sequences in batch_X\n",
        "            X_padded = pad_sequence(batch_X, batch_first=True)\n",
        "            yield X_padded, torch.tensor(batch_y, dtype=torch.long)\n",
        "            batch_X = []\n",
        "            batch_y = []\n",
        "\n",
        "    if batch_X:\n",
        "        X_padded = pad_sequence(batch_X, batch_first=True)\n",
        "        yield X_padded, torch.tensor(batch_y, dtype=torch.long)\n",
        "\n",
        "\n",
        "def train(\n",
        "        vocab,\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        context_len,\n",
        "        hidden_dim,\n",
        "        learning_rate,\n",
        "        num_epochs,\n",
        "        batch_size,\n",
        "):\n",
        "    model = Model(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        context_len=context_len,\n",
        "        hidden_dim=hidden_dim,\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "\n",
        "    sequences = list(get_sequences(vocab.tokens, context_len))\n",
        "    batches = list(batch_it(sequences, batch_size, vocab))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in tqdm(batches, leave=False):\n",
        "            optimizer.zero_grad()\n",
        "            log_probs = model(X_batch)\n",
        "            loss = loss_fn(log_probs.view(-1, model.vocab_size), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Epoch {epoch + 1}/{num_epochs}] | Loss: {total_loss / len(sequences)}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0vgKTU_HLXXR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary(tokenize(lab.train_text))\n",
        "\n",
        "model = train(\n",
        "    vocab=vocab,\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=64,\n",
        "    context_len=10,\n",
        "    hidden_dim=128,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=20,\n",
        "    batch_size=256\n",
        ")"
      ],
      "metadata": {
        "id": "tFb93XK8ErR7",
        "outputId": "e97e4fc9-dca0-4b20-9290-5b8d90375dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/20] | Loss: 0.008942597024767822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2/20] | Loss: 0.007968766632973393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3/20] | Loss: 0.007645278998222103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4/20] | Loss: 0.0074587149498837975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5/20] | Loss: 0.00733488782817987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6/20] | Loss: 0.007244172021525596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7/20] | Loss: 0.007174992296078447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8/20] | Loss: 0.007118672623347413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9/20] | Loss: 0.007071184887082296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10/20] | Loss: 0.00702978024898916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 11/20] | Loss: 0.00699347060854737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 12/20] | Loss: 0.00696160480250528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 13/20] | Loss: 0.006933546004644892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 14/20] | Loss: 0.006908525061398041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 15/20] | Loss: 0.006886438115456396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 16/20] | Loss: 0.006866633839254427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 17/20] | Loss: 0.006848597898688142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 18/20] | Loss: 0.006831793031967862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 19/20] | Loss: 0.006815969714795329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 20/20] | Loss: 0.006801056815374598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ß–∏—Ç–∞—î–º–æ –º—ñ–∂ —Ä—è–¥–∫—ñ–≤"
      ],
      "metadata": {
        "id": "USBCYglkn_WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import collections\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "# –î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω—Ü—ñ—è:\n",
        "def get_letter(w1: str, w2: str) -> str:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î –ª—ñ—Ç–µ—Ä—É, —è–∫–æ—ó –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è —Å–ª–æ–≤–∞ –∞–±–æ –ø—Ä–æ–±—ñ–ª –¥–ª—è –æ–¥–Ω–∞–∫–æ–≤–∏—Ö —Å–ª—ñ–≤.\n",
        "    \"\"\"\n",
        "\n",
        "    letters1 = collections.Counter(w1)\n",
        "    letters2 = collections.Counter(w2)\n",
        "\n",
        "    diff = letters1 - letters2\n",
        "    if len(diff) != 1:\n",
        "        return \" \"\n",
        "\n",
        "    return diff.most_common()[0][0]\n",
        "\n",
        "\n",
        "def score_text(text: str, model, vocab) -> float:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î perplexity –∞–±–æ log-probability –¥–ª—è —Ç–µ–∫—Å—Ç—É. \"\"\"\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "    total_log_prob = 0.0\n",
        "\n",
        "    for context, target in get_sequences(tokens, model.context_len):\n",
        "        X = vectorize(context, vocab)\n",
        "        target = vectorize([target], vocab)[0]\n",
        "        log_probs = model(X)\n",
        "        target_log_prob = log_probs[0, target]\n",
        "        total_log_prob += target_log_prob\n",
        "\n",
        "    return torch.exp(-torch.tensor(total_log_prob) / len(tokens)).item()\n",
        "\n",
        "# –ú–æ–∂–µ—Ç–µ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–∞ –≤–µ—Å—å —Ü–µ–π –∫–æ–¥, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n",
        "def solve(model, vocab, test_items) -> Tuple[List[str], str]:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑ —Ç–µ–∫—Å—Ç—ñ–≤ –≤ test_items —Ç–∞\n",
        "    —Å–µ–∫—Ä–µ—Ç–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "    \"\"\"\n",
        "\n",
        "    choices = []\n",
        "    secret = []\n",
        "\n",
        "    for item in test_items:\n",
        "        scores = []\n",
        "        for corr in item['corrections']:\n",
        "\n",
        "            # –ü—ñ–¥—Å—Ç–∞–≤–ª—è—î–º–æ —Å–ª–æ–≤–æ-–∫–∞–Ω–¥–∏–¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç\n",
        "            text = item['text'][:item['error_start']] + corr + item['text'][item['error_end']:]\n",
        "\n",
        "            # –†–∞—Ö—É—î–º–æ score —Ç–µ–∫—Å—Ç—É\n",
        "            score = score_text(text, model, vocab)\n",
        "            scores.append(score)\n",
        "\n",
        "            # print(f'{score:.4f} {text}')\n",
        "\n",
        "        # –°–æ—Ä—Ç—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –Ω–∞ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∑–∞ score\n",
        "        result = sorted(zip(scores, item['corrections']), key=lambda x: x[0])\n",
        "\n",
        "        # –û–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –∑–∞–º—ñ–Ω—É\n",
        "        best = result[0]\n",
        "        best_word = best[1]\n",
        "        choices.append(best_word)\n",
        "\n",
        "        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —á–µ—Ä–≥–æ–≤—É –ª—ñ—Ç–µ—Ä—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "        error = item['error']\n",
        "        letter = get_letter(error, best_word)\n",
        "        secret.append(letter)\n",
        "\n",
        "    secret_message = ''.join(secret)\n",
        "\n",
        "    return choices, secret_message\n",
        "\n",
        "choices, secret_message = solve(model, vocab, lab.test_items)\n",
        "\n",
        "lab.evaluate_accuracy(choices)\n",
        "print(\"SECRET MESSAGE: \", secret_message)\n"
      ],
      "metadata": {
        "id": "suXDePt3pLZ7",
        "outputId": "532a02af-1f7c-4c06-85e9-81badbbb4d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-6cc23526443e>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.exp(-torch.tensor(total_log_prob) / len(tokens)).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "accuracy = 0.66. –ù–µ–ø–æ–≥–∞–Ω–æ! –°–ø—Ä–æ–±—É–π —Ä–æ–∑–≥–∞–¥–∞–π –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
            "SECRET MESSAGE:   –ó–π –ø—Ä–∞–≤–¥–∞ –∫—Ä –ª–∞     —Ä—É–Ω—Ç—É –Ω  —Ç–†–µ–±–∞–∂–∑–µ–º   –ù–µ –∞—î—Ä—Ç–æ—Ä —É–¥–µ  –µ–±–æ –Ω–µ    –ø –ª—è —Ç  –± –¥–µ  –æ–ª—è –Ω–µ–º   –ü–∞—Ä–∏ —Ç–æ  —É–¥—É   —Ö –∞—Ä–∏   —Ü—å–æ–º   –∞–ø–µ–≤   –ø—Ä–∞ –¥  –ø—Ç —à–∏–Ω–∞ –∞ —è–∫  –µ –ª—é–¥ –Ω–∞ –∞ —â–æ –∂ –ª –¥ –Ω–∞   –≤–µ –Ω    –º–ª—ñ  –∞–º–∞—Ä   –ª—ñ—Ç–∞—î   –∫ –∏ –∞ –º–∞    –∫—Ä –ª–∞ –º —î–∏–≤–æ–Ω–∏ —Ç—ñ  —Ä –ª–∞ –Ω  –∑ –ø—É —É–ø —Ä'  –∞—Ä–∑ –ø—Ä –≤ –∏ —á–µ—Å –æ   —ñ –¥–æ–≤ —Ä'  —É—Ç–∫     –∑ –≤—ñ—Ä–Ω–æ—Å—Ç—ñ —É   —Ö–∞–Ω–Ω—ñ —É –∫–æ–≥–æ–º –∑—Ç —ñ—á–Ω–æ–≥–æ—ñ–ü —Ä–∏–≤–∞–Ω–∑—è—ñ—É  –æ–≥  –∑–∑–æ—â —Ä–æ—Å—Ç –æ –æ –† –± —Ç–∏ —É–±–∫–û–≥–æ  –∑  –µ —Ä–æ–°—Ç  –Ω  —Ç—É—Ä–±–æ—Ç–∏   –∫–æ–≥–æ –ø–∑ –ø—ñ–°–Ω—ñ –∞–±–æ –∑  –∞ —ñ—ó –∞–± –µ–∑ –ø–æ–µ–∑—ñ—ó     –ó –º—Ä—ñ—ó–æ–ª—é–¥–∏–Ω   —ñ –∏ –æ—á–Ω–µ   —Ç —î–æ  –∫ –∏–ª  –º–∞  –∞–∞–∫—Ä –ª –∫ –∞ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.answer(\"–õ—ñ–Ω–∞ –ö–æ—Å—Ç–µ–Ω–∫–æ\")"
      ],
      "metadata": {
        "id": "S7Hmxf6YqUVR",
        "outputId": "e21ebc57-03b9-45b4-b3b6-6dc4d89593c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "–ü—Ä–∞–≤–∏–ª—å–Ω–æ! üöÄ –ó–∞–ø–æ–≤–Ω–∏ —Ç–µ–ø–µ—Ä —Ü—é —Ñ–æ—Ä–º—É, –±—É–¥—å –ª–∞—Å–∫–∞: https://tally.so/r/wkl0zZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í—ñ–¥–ø—Ä–∞–≤—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ü–µ–π colab –∞–±–æ PDF –∑ –Ω–∏–º –Ω–∞ –ø–æ—à—Ç—É oleksii.o.syvokon@lpnu.ua. –î—è–∫—É—é!\n"
      ],
      "metadata": {
        "id": "VpE_-wtdpkHe"
      }
    }
  ]
}